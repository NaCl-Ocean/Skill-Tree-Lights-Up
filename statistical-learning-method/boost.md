# 提升方法

- **核心思想：将弱可学习算法提升为强可学习算法**
  - 弱可学习算法：正确率比随机猜测略好
  - 强可学习算法：正确率很高
  - 在PAC框架下，强可学习算法等价于弱可学习算法
- **集成学习**：
  - 序列方法
    - 提升方法
  - 并行方法



## Adaboost

- 二分类
- **基本思想：将学习的过程分成不同的stage（序列方法），每个stage去学习一个单独的base learner，训练数据的权重会发生改变， 权重的改变原则：提高在该stage下学到的base learner 误分类的数据的权重，降低正确分类的数据的权重**

### 算法流程

- 输入：训练数据集$T={(x_1,y_1),(x_2,y_2),......,(x_N,y_N)}$，$y \in {-1,+1}$，弱可学习算法(*base learners*)

- 输出：最终分类器$G(x)$

  1. **初始化**：假定第一次训练时，样本均匀分布权值一样。
        $$
        𝐷1=(𝑤_{11},𝑤_{12},𝑤_{13}......𝑤_{1𝑛}).       其中𝑤_{1𝑖}=\frac1N, 𝑖=1,2,3...N
        $$

  2. **循环**：m=1,2,3...M，

     1. 使用具有权值分布$𝐷_𝑚$的训练数据集学习，得到基本分类器$𝐺_𝑚$

     2. **计算$𝐺_𝑚(𝑥)$在训练集上的分类误差率$𝑒_𝑚$**
     $$
     		 e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^{n}w_{mi}I(G_m(x_i)\neq y_i)
        $$
        
     由上述式子可知，$𝐺_𝑚(𝑥)$在训练数据集上的误差率$𝑒_𝑚$就是被$𝐺_𝑚(𝑥)$误分类样本的权值之和。

     3. **计算𝐺𝑚(𝑥)的系数$\alpha_m$**，$\alpha_m$表示𝐺𝑚(𝑥)在最终分类器中的重要程度：
        $$
        \alpha_m = \frac{1}{2}ln\frac{1-e_m}{e_m}
        $$
        

          【注】显然𝑒𝑚<=1/2时，𝑎𝑚>=0，且𝛼𝑚随着𝑒𝑚的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大

     4. **更新训练数据集的权值分布**，用于下一轮迭代。
        $$
        D_{m+1}=(w_{m+1,1},w_{m+1,2},w_{m+1,3},...w_{m+1,n}) \\
        w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-y_i\alpha_mG_m(x_i))\\
        Z_m=\sum_{i=1}^{n}w_{mi}exp(-y_i\alpha_mG_m(x_i))
        $$
        

        其中𝑍𝑚是规范化因子，使得𝐷𝑚+1成为一个概率分布。

  3. **最终的分类器**
     $$
     f(x)=\sum_{m=1}^{M}\alpha_mG_m(x) \\
     G(x)=sign(f(x))=sign(\sum_{i=1}^{M}\alpha_mG_m(x))
     $$

### 理论支撑

- Adaboost的理论支撑很强

- Adaboost的训练误差上界
  $$
  \frac{1}{N}\sum_{i=1}^{N}I(G(x_i) \neq y_i) \leq \frac1N \sum_i exp(-y_if(x_i)) = \prod_m Z_m
  $$

- Adaboost的二分类误差上界
  $$
  \prod_m Z_m =\prod_{m=1}^{M}[2 \sqrt{e_m(1-e_m)}] = \prod_{m=1}^{M}\sqrt{(1-4\gamma^2_m)} \leq exp(-2\sum_{m=1}^{M}\gamma_m^2)  \ \ \ \   \gamma_m = \frac12 - e_m
  $$

  - 这里的证明查阅《统计学习方法》p161~162
  - **训练误差随着训练轮数的增加指数下降**

- Adaboost是**模型为加法模型**，**损失函数为指数函数**，**学习算法为前向分布算法**的二分类学习方法
  $$
  f(x) = \sum_{m=1}^{M} \beta_mb(x;\gamma_m)  \\\\
  L(y,f(x)) = exp[-yf(x)]
  $$
  

#### 前向分布算法

$f_m(x) = f_{m-1}(x) + \beta_{m}b(x; \gamma_m)$  $\gamma_m$为第m个基本分类器的参数，$\beta_m$为其对应的权重

1. 初始化 $f_0(x) = 0$

2. 对 m=1,2,.....,M，循环

   1. 极小化损失函数，得到参数$\gamma_m$和$\beta_m$
      $$
      \beta_m, \gamma_m = argmin_{\beta, \gamma} \sum_{i=1}^{N}L(y_i, f_{m-1}(x_i) + \beta b(x_i;\gamma))
      $$

   2. 更新$f_m(x)$
      $$
      f_m(x) = f_{m-1}(x) + \beta_m b(x;\gamma_m)
      $$

3. 最终模型
   $$
   f(x) = f_M(x) = \sum_{m=1}^{M} \beta_m (x;\gamma_m)
   $$

- 不同的地方在于每一轮不是单独去学习一个base learner，而是在学一个整体。

## 提升树

- Base learner是 决策树

### 分类树

- Base learner是 分类决策树
- 分类树的策略与Adaboost相同，将Adaboos的基本分类器换成分类决策树就可以

### 回归树

- Base learner是 回归决策树
- 对于回归树，一般用梯度提升算法来对任意可微分的损失函数的回归树进行优化
- 如果损失函数为平方损失，那么可以进一步简化。



#### 梯度提升算法

- 初始化$f_0(x) = argmin_c \sum_{i=1}^{N} L(y_i,c)$ 
  - 初始化，此时为只有一个根节点的树，取常数使损失函数最小
- 对$m=1,2,3.....,M$
  - 对于$i = 1,2,3,......,N$ , 计算

