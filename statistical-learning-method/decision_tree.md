# 决策树

- 多分类模型

- 概念

  - 决策树由节点和有向边组成
    - 节点：内部节点（表示一个特征或属性），叶节点（表示一个类）
  - if-then 规则的集合

- 三个关键步骤

  - **特征选择**

    - 什么是特征选择：选择一个特征，使得可以很好地分类，如果根据某个特征进行选择，与随机分类的结果没有差别，那么这个特征就是没有分类能力的

    - 通常基于以下三种准则

      - **信息增益(information gain)最大化**    集合D的熵与给定特征A的条件下集合D的熵之差（这里集合D的熵也就是对于**label**这个随机变量计算熵）

        - $$
          g(D,A) = H(D) - H(D|A)\ \ H(D|A)=\sum_{i=1}^{n}p_iH(D|A=a_i)
          $$

      - **信息增益比(information gain ratio) 最大化** 信息增益与数据集D关于特征值A的值的上$H_A(D)$之比。（这里的$H_A(D)$实际上就是对于特征值A这个随机变量计算熵）

      - $$
        g_R(D,A) = \frac{g(D,A)}{H_A(D)}
        $$

        

      - **基尼指数(ginni coefficient) 最小化** 

        - 基尼指数的定义，对于离散的随机变量K，则K的基尼指数定义为

          - $$
            Gini(p) = \sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
            $$

        - 给定特征A以及A对应的一个a，则集合D可以被划分为两个集合$D_1=\{x \in D| x_A = a \}$和$D_2= \{x \in D |x_A \neq a\}$ ，则在给定特征A以及对应的a之后，集合D的基尼指数为

          - $$
            Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
            $$

            

  - **决策树的生成**

    - 决策树生成就是利用上述准则不断地生成子树的过程，主要有3种方法：**ID3, C4.5, CART**，可以抽象为以下算法流程
    - 输入：训练数据集D，特征集A，阈值$\sigma$，特征选择准则mertic
    - 输出：决策树T
      1. 递归地构造子树：
         1. 如果D中所有实例属于同一类$C_k$，则T为单节点树，将$C_k$作为树的输出，返回T；如果特征集$A = \emptyset $，那么T为单节点树，将D中实例数最多的类$C_k$作为树的输出。
         2. 在当前特征集A中，对于数据集D，选择一个对于特征选择准则来说**最优的特征**$A_g$，并将D中实例数最多的类作为该节点的标记(为了剪枝)。
            1. 对于**CART**来说，稍微不一样的是，要选择的是**最优特征$A_g$及其对应的特征值$a$**，因为CART构造的是一个二叉树，特征$A_g$等于$a$，那么往下走到左节点，不等于，则走到右节点。
         3. 如果根据该特征选择准则计算的不确定度减少量小于阈值，那么当前节点为单节点树，将D中实例数最多的类作为输出。
            1. 对于ID3和C4.5来说，也就是信息增益或者信息增益比小于阈值
            2. 对于CART来说，也就是基尼指数大于阈值
            3. 这里实际上是一个预剪枝的步骤
         4. 对于$A_g$的每一个特征值$a_i$，依据$A_g = a_i$将D分割为若干个非空子集$D_i$，当前节点的子节点即为根据$D_i$ 和特征集$A-A_g$构造的子树。

  - **剪枝**

    - 一般而言，生成的决策树往往会比较复杂，在训练集上拟合得很好，但是由于比较复杂，也就是说分支太多，往往会发生过拟合，这时候我们就需要减小模型的复杂度，以提高在测试集上的精度。也就是进行剪枝。

    - 决策树的损失函数 

      - $$
        C_{\alpha}(T) = C(T) + \alpha |T|
        $$

        

      - T 为所有的叶节点，所有的叶节点个数为$|T|$，每个叶节点有$N_t$个样本点

      - $$
        C(T) = \sum_{t=1}^{|T|}N_tH_t(T)
        $$

        - $H_t(T)$是每个叶节点的经验熵，将每个叶节点上样本点的数目作为权重，所有的叶节点求和。

    - 具体算法

      - 计算每个节点的经验熵，递归地从树的叶节点向上回缩

      - 设一组叶节点回缩到其父节点之前与之后的**整体树的损失函数**分别为$T_B$与$T_A$,其对应的损失函数分别为$C_{\alpha}(T_B)$与$C_{\alpha}(T_A)$，如果$C_{\alpha}(T_A) \leq C_{\alpha}(T_B)$，那么将父节点作为新的子节点。

      - [不懂的话，再看看这里](https://blog.csdn.net/u012328159/article/details/79285214)

        